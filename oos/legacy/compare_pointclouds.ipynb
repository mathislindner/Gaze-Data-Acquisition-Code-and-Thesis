{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colmap_testing.colmap_helpers import read_write_model\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from constants import *\n",
    "from matplotlib import pyplot as plt\n",
    "import open3d as o3d\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_id = \"ff0acdab-123d-41d8-bfd6-b641f99fc8eb\"\n",
    "\n",
    "pointcloud_depth_camera_path = os.path.join(recordings_folder, recording_id, \"depth_camera_pointcloud.ply\")\n",
    "meshed_depth_camera_path = os.path.join(recordings_folder, recording_id,\"meshed_depth_camera.ply\")\n",
    "pointcloud_colmap_path =os.path.join(recordings_folder,\"/scratch_net/snapo/mlindner/docs/gaze_data_acquisition/mathis/recordings/ff0acdab-123d-41d8-bfd6-b641f99fc8eb/colmap_ws/automatic_recontructor/dense/0/meshed-poisson.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_file_path = os.path.join(recordings_folder, recording_id, \"realsensed435.bag\")\n",
    "# First import the library\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "\n",
    "# Declare pointcloud object, for calculating pointclouds and texture mappings\n",
    "pc = rs.pointcloud()\n",
    "# We want the points object to be persistent so we can display the last cloud when a frame drops\n",
    "points = rs.points()\n",
    "\n",
    "# Declare RealSense pipeline, encapsulating the actual device and sensors\n",
    "pipe = rs.pipeline()\n",
    "config = rs.config()\n",
    "# Enable depth stream\n",
    "#config.enable_stream(rs.stream.depth)\n",
    "config.enable_device_from_file(bag_file_path, repeat_playback=False)\n",
    "\n",
    "\n",
    "# Start streaming with chosen configuration\n",
    "profile = pipe.start(config)\n",
    "playback = profile.get_device().as_playback()\n",
    "playback.set_real_time(False)\n",
    "# We'll use the colorizer to generate texture for our PLY\n",
    "# (alternatively, texture can be obtained from color or infrared stream)\n",
    "colorizer = rs.colorizer()\n",
    "\n",
    "try:\n",
    "    # Wait for the next set of frames from the camera\n",
    "    frames = pipe.wait_for_frames()\n",
    "    colorized = colorizer.process(frames)\n",
    "\n",
    "    # Create save_to_ply object\n",
    "    ply = rs.save_to_ply(pointcloud_depth_camera_path)\n",
    "\n",
    "    # Set options to the desired values\n",
    "    # In this example we'll generate a textual PLY with normals (mesh is already created by default)\n",
    "    ply.set_option(rs.save_to_ply.option_ply_binary, False)\n",
    "    ply.set_option(rs.save_to_ply.option_ply_normals, True)\n",
    "    ply.set_option(rs.save_to_ply.option_ply_mesh, False)\n",
    "\n",
    "    print(\"Saving to 1.ply...\")\n",
    "    # Apply the processing block to the frameset which contains the depth frame and the texture\n",
    "    ply.process(colorized)\n",
    "    print(\"Done\")\n",
    "finally:\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = \"/scratch_net/snapo/mlindner/docs/gaze_data_acquisition/mathis/recordings/ff0acdab-123d-41d8-bfd6-b641f99fc8eb\"\n",
    "\n",
    "pointcloud_depth_camera = o3d.io.read_point_cloud(os.path.join(recording,\"depth_camera_pointcloud.ply\"))\n",
    "pointcloud_colmap = o3d.io.read_point_cloud(os.path.join(recording,\"/scratch_net/snapo/mlindner/docs/gaze_data_acquisition/mathis/recordings/ff0acdab-123d-41d8-bfd6-b641f99fc8eb/colmap_ws/automatic_recontructor/dense/0/fused.ply\"))\n",
    "\n",
    "# Extract key features\n",
    "keypoints1 = np.asarray(pointcloud_depth_camera.points)\n",
    "keypoints2 = np.asarray(pointcloud_colmap.points)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#↨inital guess: x_rotation = 180°, y_rotation = 0°, z_rotation = 0°, x_translation = 0, y_translation = 0, z_translation = 0, scale = 4\n",
    "initial_matrix = np.array([[4, 0, 0, 0],\n",
    "                           [0, -4, 0, 0],\n",
    "                           [0, 0, -4, 0],\n",
    "                           [0, 0, 0, 0]])\n",
    "\n",
    "# Use Iterative Closest Point algorithm to find the best alignment\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(pointcloud_depth_camera, pointcloud_colmap, max_correspondence_distance = 0.5, init = initial_matrix)\n",
    "\n",
    "\n",
    "transformation = reg_p2p.transformation\n",
    "print(transformation)\n",
    "# Apply the transformation matrix to the second point cloud\n",
    "\n",
    "keypoints1 = keypoints1 @ transformation[:3,:3].T + transformation[:3,3]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gazeacq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
